Teams traditionally measure content in terms of engagement and clarity. And that's totally OK, it's a necessary part of the measuring. The point I'll try to make is that by stopping there, it becomes difficult to relate content to business outcomes, which is the real goal of content. You know the say “People don’t want to buy a quarter-inch drill. They want a quarter-inch hole!"? Well, similarly, people don't want to engage with content, they want their questions answered and their problem solved, so that they can complete their jobs and move on with their days. By reframing content measuring from this perspective, we create new spaces to link content performance to behavior change. When I say user behavior I'm talking about feature discovery, feature adoption, and feature proficiency. With the mapping in place, we can define goals for each stage. For example, awareness requires that users are informed about the feature, realize its potential by understanding how to improve their lives. But also, this step must drive to the next step. Once they are aware, they may be exploring how it works: what can we give them to get a certain feature closer to a problem? Is it by upselling in the UI, is it by giving them a Get Started article linked to the release notes? What information do they need to think "oh this is what I wanted to know now and this is what I'd like to know next?". But also realize that not all features require activation, some just need awareness, and that's fine. The important part is being able to assess this in a systematic way, and to have feedback loops in place to learn and iterate.

We want to start analyzing the task completion rate for a certain JTBD, how many errors or support queries (on how to do things, for example) a certain workflow generates, and start optimizing content towards that. If you're into AI agents trained on your docs, you could measure for example, containment/deflection rate (can the chatbot give the answer users are looking for without having to escalate to support?) or, if there are the technical means to do it, you could measure support tickets of trained users vs untrained users (whatever "trained" means for you)

Measuring the operational side remains important to create efficient ways of working, so we shouldn't stop (or should start) measuring: Roadmapping-to-publish cycles, Review time, Adherence to standards -> And that's also where we can act from an operational perspective and remove friction (but this is something for task 2)