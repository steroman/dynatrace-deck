Here's the narrative I want to build:

As we briefly touched in previous slides, among the advantages of building standards is that deviation from them can be measured.

There's no single process but a combination of processes: There's the process of automating the identification of content to produce at each increment. That can be done through the mapping and the tiering. Once that happens, automations can be built in your work tracking tools of choice can be built that trigger work when a certain item enters or moves through the roadmap. Forms and metadata can be built around work items to ensure that information is collected and provided upfront.

More organic interactions count too (they actually count more than we imagine), so can we measure those too? Are we considering all the workstreams?

As I mentioned before, I'm envisioning this as a mindset shift where every increment is a delta to the status quo. It's a bit like git works, any CRUD action to the product results in a work item, which triggers a certain set of content-related actions. This mainly helps with content staleness. I've worked in teams where periodic staleness checks were set up, like "Throw an outdatedness notification every 6 months that a certain article hasn't been touched". That can work too, but if we tackle it from a mere task completion perspective, we'll incentivize content owners to act quickly without properly reevaluating whether content is stale. So I'm more about the "git" approach to software increments + random QA checks. Or, if we want to get even more standardize, there's docs as tests, where each doc is supported by e2e tests that, when failing, convey staleness.

Ownership and accountability also matter in all of this. So ability to influence definitions of ready, definitions of done, is important too.

But it's also about considering all possible workstreams, and how to govern the content.

Then there's the part of balancing day-to-day with more long-term tasks.

Then there's the part of turning north star business outcomes into actionable items. North star > OKRs > GIST > RICE > Evidence Score > Roadmap. Create a culture of experimentation. But other frameworks can be used obv, the importance here is that we get in the mindset of becoming evidence based as well as building a culture of experimentation and frequent reassessment.

There's other governance: how often do we evaluate how content is performing, are there article that perform poorly? Why?